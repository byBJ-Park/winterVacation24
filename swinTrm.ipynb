{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "x_train = tf.cast(x_train, tf.float32)\n",
    "x_test  = tf.cast(x_test,  tf.float32)\n",
    "\n",
    "x_train = tf.image.resize(x_train, [72, 72])  # (50000, 72, 72, 3)\n",
    "x_test  = tf.image.resize(x_test,  [72, 72])  # (10000, 72, 72, 3)\n",
    "\n",
    "\n",
    "x_train /= 255.0\n",
    "x_test  /= 255.0\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "dataget = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_generator = dataget.flow(x_train, y_train, batch_size=32, subset='training')\n",
    "validation_generator = dataget.flow(x_train, y_train, batch_size=32, subset='validation')\n",
    "\n",
    "for x_batch, y_batch in train_generator:\n",
    "    print(f\"Train Generator: {x_batch.shape}, {y_batch.shape}\")\n",
    "    break\n",
    "for x_batch, y_batch in validation_generator:\n",
    "    print(f\"Validation Generator: {x_batch.shape}, {y_batch.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_partition(x, window_size):\n",
    "  B, H, W, C = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n",
    "  pad_h = (window_size - H % window_size) % window_size\n",
    "  pad_w = (window_size - W % window_size) % window_size\n",
    "  x = tf.pad(x, [[0, 0], [0, pad_h], [0, pad_w], [0, 0]])\n",
    "  H_padded, W_padded = H + pad_h, W + pad_w\n",
    "\n",
    "  x = tf.reshape(x, [B, H_padded // window_size, window_size, W_padded // window_size, window_size, C])\n",
    "  x = tf.transpose(x, [0, 1, 3, 2, 4, 5])\n",
    "  x = tf.reshape(x, [-1, window_size * window_size, C])\n",
    "\n",
    "  return x, H_padded, W_padded\n",
    "\n",
    "def window_reverse(windows, window_size, H_padded, W_padded, H, W, C):\n",
    "  B = tf.shape(windows)[0] // (H_padded // window_size * W_padded // window_size)\n",
    "  x = tf.reshape(windows, [B, H_padded // window_size, W_padded // window_size, window_size, window_size, C])\n",
    "  x = tf.transpose(x, [0, 1, 3, 2, 4, 5])\n",
    "  x = tf.reshape(x, [B, H_padded, W_padded, C])\n",
    "  x = x[:, :H, :W, :]\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAttention(layers.Layer):\n",
    "  def __init__(self, dim, window_size, num_heads):\n",
    "    super(WindowAttention, self).__init__()\n",
    "    self.dim = dim\n",
    "    self.window_size = window_size\n",
    "    self.num_heads = num_heads\n",
    "    assert dim % num_heads == 0, \"dim must be divisible by num_heads\"\n",
    "    self.head_dim = dim // num_heads\n",
    "    self.scale = tf.math.pow(tf.cast(self.head_dim, tf.float32), -0.5)\n",
    "\n",
    "    self.query = layers.Dense(dim, use_bias=False)\n",
    "    self.key = layers.Dense(dim, use_bias=False)\n",
    "    self.value = layers.Dense(dim, use_bias=False)\n",
    "    self.proj = layers.Dense(dim)\n",
    "\n",
    "  def call(self, x, mask=None):\n",
    "    B_, N, C = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2]\n",
    "\n",
    "    tf.debugging.assert_equal(C, self.dim, message=f\"Input dimension ({C}) doesn't match layer dimension ({self.dim}).\")\n",
    "    tf.debugging.assert_equal(N, self.window_size ** 2, message=f\"Input sequence length ({N}) doesn't match window size ({self.window_size}).\")\n",
    "\n",
    "    q = tf.reshape(self.query(x), [B_, N, self.num_heads, self.head_dim])\n",
    "    k = tf.reshape(self.key(x), [B_, N, self.num_heads, self.head_dim])\n",
    "    v = tf.reshape(self.value(x), [B_, N, self.num_heads, self.head_dim])\n",
    "\n",
    "    q = tf.transpose(q, [0, 2, 1, 3]) * self.scale\n",
    "    k = tf.transpose(k, [0, 2, 1, 3])\n",
    "    v = tf.transpose(v, [0, 2, 1, 3])\n",
    "\n",
    "    attn = tf.matmul(q, k, transpose_b=True)\n",
    "    if mask is not None:\n",
    "      attn += mask\n",
    "    aatn = tf.nn.softmax(attn, axis=-1)\n",
    "\n",
    "    out = tf.matmul(aatn, v)\n",
    "    out = tf.transpose(out, [0, 2, 1, 3])\n",
    "    out = tf.reshape(out, [B_, N, C])\n",
    "    out = self.proj(out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerBlock(layers.Layer):\n",
    "  def __init__(self, dim, num_heads, window_size=7, shift_size=0, mlp_ratio=4.0):\n",
    "    super(SwinTransformerBlock, self).__init__()\n",
    "    self.dim = dim\n",
    "    self.num_heads = num_heads\n",
    "    self.window_size = window_size\n",
    "    self.shift_size = shift_size\n",
    "    self.mlp_ratio = mlp_ratio\n",
    "\n",
    "    self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n",
    "    self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n",
    "    self.attn = WindowAttention(dim, window_size, num_heads)\n",
    "    mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "    self.mlp = models.Sequential([\n",
    "            layers.Dense(mlp_hidden_dim, activation='gelu'),\n",
    "            layers.Dropout(0.4),# dropout\n",
    "            layers.Dense(dim),\n",
    "            layers.Dropout(0.4) # dropout\n",
    "        ])\n",
    "\n",
    "  def call(self, x):\n",
    "    shortcut = x\n",
    "    H, W, C = tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n",
    "    x = self.norm1(x)\n",
    "    x_windows, H_padded, W_padded = window_partition(x, self.window_size)\n",
    "    attn_windows = self.attn(x_windows)\n",
    "    x = window_reverse(attn_windows, self.window_size, H_padded, W_padded, H, W, C)\n",
    "    x = x + shortcut\n",
    "    shortcut = x\n",
    "    x = self.norm2(x)\n",
    "    x = self.mlp(x)\n",
    "    x = x + shortcut\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer(models.Model):\n",
    "  def __init__(self, img_size=32, patch_size=4, num_classes=10, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24]):\n",
    "    super(SwinTransformer, self).__init__()\n",
    "    self.patch_embed = layers.Conv2D(96, kernel_size=patch_size, strides=patch_size, padding='valid')\n",
    "    self.pos_drop = layers.Dropout(0.2)\n",
    "    self.blocks = []\n",
    "    dim = 96\n",
    "    for i in range(len(depths)):\n",
    "      for j in range(depths[i]):\n",
    "        shift_size = 0 if j % 2 == 0 else patch_size // 2\n",
    "        self.blocks.append(\n",
    "            SwinTransformerBlock(\n",
    "                dim=dim,\n",
    "                num_heads=num_heads[i],\n",
    "                window_size=7,\n",
    "                shift_size=shift_size\n",
    "            )\n",
    "        )\n",
    "      if i < len(depths) - 1:\n",
    "        dim *= 2\n",
    "        self.blocks.append(layers.Conv2D(dim, kernel_size=2, strides=2, padding='valid'))\n",
    "    self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.avgpool = layers.GlobalAveragePooling2D()\n",
    "    self.head = layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.patch_embed(x)\n",
    "    x = self.pos_drop(x)\n",
    "    for block in self.blocks:\n",
    "      x = block(x)\n",
    "    x = self.norm(x)\n",
    "    x = self.avgpool(x)\n",
    "    x = self.head(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = tf.random.normal([64, 32, 32, 3])\n",
    "\n",
    "patch_embed_output = layers.Conv2D(96, kernel_size=4, strides=4, padding='valid')(sample_input)\n",
    "print(f\"Patch Embedding Output Shape: {patch_embed_output.shape}\")\n",
    "\n",
    "windows, H_padded, W_padded = window_partition(patch_embed_output, 7)\n",
    "print(f\"Windowed Patches Shape: {windows.shape}\")\n",
    "\n",
    "restored_output = window_reverse(windows, window_size=7, H_padded=H_padded, W_padded=W_padded,\n",
    "                                 H=tf.shape(patch_embed_output)[1], W=tf.shape(patch_embed_output)[2], C=tf.shape(patch_embed_output)[3])\n",
    "print(f\"Restored Output Shape: {restored_output.shape}\")\n",
    "\n",
    "swin_block  = SwinTransformerBlock(dim=96, num_heads=3, window_size=7) # layers.Sequential -> models.Sequential\n",
    "block_output = swin_block(patch_embed_output)\n",
    "print(f\"Swin Transformer Block Output Shape: {block_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "model = SwinTransformer(img_size=72, patch_size=4, num_classes=10)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4, weight_decay=1e-5),\n",
    "    loss=CategoricalCrossentropy(label_smoothing=0.1),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=100,\n",
    "    callbacks=[lr_reducer]\n",
    ")\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Progress')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy Progress')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
